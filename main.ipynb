{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import datetime\n",
    "# import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tweepy as tw\n",
    "import threading\n",
    "import time\n",
    "import queue\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    return pd.DataFrame(\n",
    "        columns=[\n",
    "            \"tweet_id\",\n",
    "            \"name\",\n",
    "            \"screen_name\",\n",
    "            \"retweet_count\",\n",
    "            \"text\",\n",
    "            \"mined_at\",\n",
    "            \"created_at\",\n",
    "            \"favourite_count\",\n",
    "            \"hashtags\",\n",
    "            \"status_count\",\n",
    "            \"followers_count\",\n",
    "            \"location\",\n",
    "            \"source_device\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Twitter API ####\n",
    "class TweetMiner(object):\n",
    "    result_minit = 20\n",
    "    ret = []\n",
    "    api = False\n",
    "\n",
    "    twitter_keys = {\n",
    "        \"consumer_key\": config.consumer_key,\n",
    "        \"consumer_secret\": config.consumer_secret,\n",
    "        \"access_token_key\": config.access_token_key,\n",
    "        \"access_token_secret\": config.access_token_secret,\n",
    "    }\n",
    "\n",
    "    def __init__(self, keys_dict=twitter_keys, api=api):\n",
    "        \"\"\" \n",
    "        Initialize the miner.\n",
    "        \"\"\"\n",
    "        self.api = api\n",
    "        auth = tw.OAuthHandler(keys_dict[\"consumer_key\"], keys_dict[\"consumer_secret\"])\n",
    "        auth.set_access_token(keys_dict[\"access_token_key\"], keys_dict[\"access_token_secret\"])\n",
    "        self.api = tw.API(auth, wait_on_rate_limit=True)\n",
    "        self.twitter_keys = keys_dict\n",
    "        self.Path = Path(f'{os.getcwd()}, tweets')\n",
    "\n",
    "    def mine_tweets(self, query=\"BTC\"):\n",
    "        \"\"\"\n",
    "        Mine tweets from the query.\n",
    "        \"\"\"\n",
    "        last_tweet_id = False\n",
    "        page_num = 1\n",
    "\n",
    "        ret = get_df()\n",
    "        crypto_query = f\"#{query}\"\n",
    "        print(\"========\", query, crypto_query)\n",
    "\n",
    "        for page in tw.Cursor(\n",
    "            self.api.search_tweets, \n",
    "            q=crypto_query, \n",
    "            lang=\"en\", \n",
    "            tweet_mode=\"extended\", \n",
    "            count=200\n",
    "        ).pages():\n",
    "            print(\"........... new page\", page_num)\n",
    "            page_num += 1\n",
    "\n",
    "            for tweet in page:\n",
    "                data = {\n",
    "                    \"tweet_id\": tweet.id,\n",
    "                    \"name\": tweet.user.name,\n",
    "                    \"screen_name\": tweet.user.screen_name,\n",
    "                    \"retweet_count\": tweet.retweet_count,\n",
    "                    \"text\": tweet.full_text,\n",
    "                    \"mined_at\": datetime.datetime.now(),\n",
    "                    \"created_at\": tweet.created_at,\n",
    "                    \"favourite_count\": tweet.favorite_count,\n",
    "                    \"hashtags\": tweet.entities.get(\"hashtags\"),\n",
    "                    \"status_count\": tweet.user.statuses_count,\n",
    "                    \"followers_count\": tweet.user.followers_count,\n",
    "                    \"location\": tweet.user.location,\n",
    "                    \"source_device\": tweet.source,\n",
    "                }\n",
    "                try:\n",
    "                    data[\"retweet_text\"] = tweet.retweeted_status.full_text\n",
    "                except:\n",
    "                    data[\"retweet_text\"] = \"None\"\n",
    "\n",
    "                last_tweet_id = tweet.id\n",
    "                ret = ret.append(data, ignore_index=True)\n",
    "\n",
    "            #print(\"Page:\", page_num)\n",
    "            if page_num % 180 == 0:\n",
    "                date_label = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                print(\"Saving to file:\", date_label)\n",
    "                ret.to_csv(f\"{date_label}-{query}.csv\", index=False)\n",
    "                print(\"Resetting df\")\n",
    "                ret = get_df()\n",
    "        date_label = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        ret.to_csv(f\"{self.Path}\\{date_label}-{query}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = TweetMiner()\n",
    "\n",
    "handle_list = [\n",
    "    #\"BTC\",\n",
    "    #\"ETH\",\n",
    "    #\"USDT\",\n",
    "    \"XRP\",\n",
    "    #\"BCH\",\n",
    "    #\"ADA\",\n",
    "    #\"BSV\",\n",
    "    #\"LTC\",\n",
    "    #\"LINK\",\n",
    "    #\"BNB\",\n",
    "    #\"EOS\",\n",
    "    #\"TRON\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_publish = threading.Event()\n",
    "update_queue = queue.Queue()\n",
    "\n",
    "\n",
    "def start_publisher():\n",
    "    \"\"\" \n",
    "    Start the publisher thread. (This is the thread that will be listening for updates)\n",
    "    \"\"\"\n",
    "    global handle_list\n",
    "\n",
    "    starttime = time.time()\n",
    "    print(\"Start polling\", starttime)\n",
    "    poll_iteration = 1\n",
    "\n",
    "    for i in range(10):\n",
    "        for name in handle_list[:1]:\n",
    "            print(i, poll_iteration, \"\\rpublishing update \", end=\"\")\n",
    "            update_queue.put((poll_iteration, name))\n",
    "            poll_iteration += 1\n",
    "            time.sleep(900)\n",
    "            print(\"\\rawaiting for publishing update\", end=\"\")\n",
    "            should_publish.wait()\n",
    "            update_queue.join()\n",
    "\n",
    "def start_update_listener():\n",
    "    \"\"\"\n",
    "    Start the update listener thread. (This is the thread that will be listening for updates)\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        poll_iteration, name = update_queue.get()\n",
    "\n",
    "        print(\" --- \", name)\n",
    "        try:\n",
    "\n",
    "            miner.mine_tweets(query=name)\n",
    "            update_queue.task_done()\n",
    "\n",
    "        except Exception as e:  # work on python 3.x\n",
    "            print(\"Failed to upload to ftp: \" + str(e))\n",
    "\n",
    "# Start the threads\n",
    "listener_thread = threading.Thread(target=start_update_listener, daemon=True)\n",
    "publisher_thread = threading.Thread(target=start_publisher, daemon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start polling 1640074843.7755203\n",
      "publishing update  ---  XRP\n",
      "======== XRP #XRP\n",
      "........... new page 1\n",
      "........... new page 2\n",
      "........... new page 3\n",
      "........... new page 4\n",
      "........... new page 5\n",
      "........... new page 6\n",
      "........... new page 7\n",
      "........... new page 8\n",
      "........... new page 9\n",
      "........... new page 10\n",
      "........... new page 11\n",
      "........... new page 12\n",
      "........... new page 13\n",
      "........... new page 14\n",
      "........... new page 15\n",
      "........... new page 16\n",
      "........... new page 17\n",
      "........... new page 18\n",
      "........... new page 19\n",
      "........... new page 20\n",
      "........... new page 21\n",
      "........... new page 22\n",
      "........... new page 23\n",
      "........... new page 24\n",
      "........... new page 25\n",
      "........... new page 26\n",
      "........... new page 27\n",
      "........... new page 28\n",
      "........... new page 29\n",
      "........... new page 30\n",
      "........... new page 31\n",
      "........... new page 32\n",
      "........... new page 33\n"
     ]
    }
   ],
   "source": [
    "publisher_thread.start()\n",
    "listener_thread.start()\n",
    "# start publishing\n",
    "should_publish.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........... new page 34\n",
      "........... new page 35\n",
      "........... new page 36\n",
      "........... new page 37\n",
      "........... new page 38\n",
      "........... new page 39\n",
      "........... new page 40\n",
      "........... new page 41\n",
      "........... new page 42\n",
      "........... new page 43\n",
      "........... new page 44\n",
      "........... new page 45\n",
      "........... new page 46\n",
      "........... new page 47\n",
      "........... new page 48\n",
      "........... new page 49\n",
      "........... new page 50\n",
      "........... new page 51\n",
      "........... new page 52\n",
      "........... new page 53\n",
      "........... new page 54\n",
      "........... new page 55\n",
      "........... new page 56\n",
      "........... new page 57\n",
      "........... new page 58\n",
      "........... new page 59\n",
      "........... new page 60\n",
      "........... new page 61\n",
      "........... new page 62\n",
      "........... new page 63\n",
      "........... new page 64\n",
      "........... new page 65\n",
      "........... new page 66\n",
      "........... new page 67\n",
      "........... new page 68\n",
      "........... new page 69\n",
      "........... new page 70\n",
      "........... new page 71\n",
      "........... new page 72\n",
      "........... new page 73\n",
      "........... new page 74\n",
      "........... new page 75\n",
      "........... new page 76\n",
      "........... new page 77\n",
      "........... new page 78\n",
      "........... new page 79\n",
      "........... new page 80\n",
      "........... new page 81\n",
      "........... new page 82\n",
      "........... new page 83\n",
      "........... new page 84\n",
      "........... new page 85\n",
      "........... new page 86\n",
      "........... new page 87\n",
      "........... new page 88\n",
      "........... new page 89\n",
      "........... new page 90\n",
      "........... new page 91\n",
      "........... new page 92\n",
      "........... new page 93\n",
      "........... new page 94\n",
      "........... new page 95\n",
      "........... new page 96\n",
      "........... new page 97\n",
      "........... new page 98\n",
      "........... new page 99\n",
      "........... new page 100\n",
      "........... new page 101\n",
      "........... new page 102\n",
      "........... new page 103\n",
      "........... new page 104\n",
      "........... new page 105\n",
      "........... new page 106\n",
      "........... new page 107\n",
      "........... new page 108\n",
      "........... new page 109\n",
      "........... new page 110\n",
      "........... new page 111\n",
      "........... new page 112\n",
      "........... new page 113\n",
      "........... new page 114\n",
      "........... new page 115\n",
      "........... new page 116\n",
      "........... new page 117\n",
      "........... new page 118\n",
      "........... new page 119\n",
      "........... new page 120\n",
      "........... new page 121\n",
      "........... new page 122\n",
      "........... new page 123\n",
      "........... new page 124\n",
      "........... new page 125\n",
      "........... new page 126\n",
      "........... new page 127\n",
      "........... new page 128\n",
      "........... new page 129\n",
      "........... new page 130\n",
      "........... new page 131\n",
      "........... new page 132\n",
      "........... new page 133\n",
      "........... new page 134\n",
      "........... new page 135\n",
      "........... new page 136\n",
      "........... new page 137\n",
      "........... new page 138\n",
      "........... new page 139\n",
      "........... new page 140\n",
      "........... new page 141\n",
      "........... new page 142\n",
      "........... new page 143\n",
      "........... new page 144\n",
      "........... new page 145\n",
      "........... new page 146\n",
      "........... new page 147\n",
      "........... new page 148\n",
      "........... new page 149\n",
      "........... new page 150\n",
      "........... new page 151\n",
      "........... new page 152\n",
      "........... new page 153\n",
      "........... new page 154\n",
      "........... new page 155\n",
      "........... new page 156\n",
      "........... new page 157\n",
      "........... new page 158\n",
      "........... new page 159\n",
      "........... new page 160\n",
      "........... new page 161\n",
      "........... new page 162\n",
      "........... new page 163\n",
      "........... new page 164\n",
      "........... new page 165\n",
      "........... new page 166\n",
      "........... new page 167\n",
      "........... new page 168\n",
      "........... new page 169\n",
      "........... new page 170\n",
      "........... new page 171\n",
      "........... new page 172\n",
      "........... new page 173\n",
      "........... new page 174\n",
      "........... new page 175\n",
      "........... new page 176\n",
      "........... new page 177\n",
      "........... new page 178\n",
      "........... new page 179\n",
      "Saving to file: 2021-12-21\n",
      "Resetting df\n",
      "........... new page 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 581\n"
     ]
    }
   ],
   "source": [
    "should_publish.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93263193325cd6f76a9e21d176003adfa7a63d8f308e38218ffc45e6019c4146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
