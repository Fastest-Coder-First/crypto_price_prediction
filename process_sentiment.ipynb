{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency = [\n",
    "    \"BTC\",\n",
    "    #\"ETH\",\n",
    "    #\"USDT\",\n",
    "    # \"XRP\",\n",
    "    #\"BCH\",\n",
    "    #\"ADA\",\n",
    "    #\"BSV\",\n",
    "    #\"LTC\",\n",
    "    #\"LINK\",\n",
    "    #\"BNB\",\n",
    "    #\"EOS\",\n",
    "    #\"TRON\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Number of records loaded for BTC 16955\n",
    "# Number of records loaded for XRP 17622\n",
    "# TOTAL: 16955 + 17622 = 34667\n",
    "###############################################################################\n",
    "\n",
    "class process_tweets():\n",
    "\n",
    "    def __init__(self, tokenizer=None, stop_words=None, stemmer=None, lemmatizer=None):\n",
    "        \"\"\" \n",
    "        Initialize the class.\n",
    "        \"\"\"\n",
    "        self.path = Path(f'{os.getcwd()}')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.stemmer = stemmer\n",
    "        self.lemmatizer = lemmatizer\n",
    "\n",
    "        self.df = []\n",
    "        self.vocabulary = []\n",
    "        self.final = []\n",
    "\n",
    "    # Read tweets from CSV for every currency\n",
    "    def read_tweets(self, curr):\n",
    "        \"\"\"\n",
    "        Read the tweets from the CSV file.\n",
    "        \"\"\"\n",
    "        #initialize the dataframe\n",
    "        ret = []\n",
    "        for file in glob.glob(f\"{self.path}/*-{curr}*.csv\"):\n",
    "            ret = pd.concat([pd.read_csv(file)], ignore_index=True)\n",
    "            ret['coin_type'] = curr\n",
    "\n",
    "        self.df.append(ret)\n",
    "\n",
    "    def clean_df(self):\n",
    "        \"\"\"\n",
    "        Since I repeted the data mining multiple times, we expect duplicate of tweets.\n",
    "        Keep the latess mined as the number of followers and retweets can chage.\n",
    "        \"\"\"\n",
    "        for index in range(len(self.df)):\n",
    "            self.df[index].sort_values(by=[\"mined_at\"], inplace=True, ignore_index=True) \n",
    "            self.df[index].drop_duplicates(\n",
    "                subset=[\"tweet_id\"], inplace=True, keep=\"last\", ignore_index=True\n",
    "            )\n",
    "\n",
    "    def process_ccy(self):\n",
    "        \"\"\"\n",
    "        Processes the currency data.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for index, curr in enumerate(self.df):\n",
    "            ret = []\n",
    "            for line in self.df[index]['text']:\n",
    "                # Initialize stem_text\n",
    "                stem_text = \"\"\n",
    "                for word in self.tokenizer.tokenize(line):\n",
    "                    count += 1\n",
    "                    word_lower = word.lower()\n",
    "                    if word not in self.stop_words:\n",
    "                        if self.lemmatizer:\n",
    "                            word_lemmatized = self.lemmatizer.lemmatize(word_lower)\n",
    "                            stem_text += word_lemmatized + \" \"\n",
    "                            self.vocabulary.append(word_lemmatized)\n",
    "                # Append the stemmed text to the list\n",
    "                ret.append(stem_text)\n",
    "            # Create a new column with the stemmed text\n",
    "            self.df[index]['text_clean'] = np.array(ret)\n",
    "\n",
    "        print(f\"Number of words: {count}\")\n",
    "        print(f\"Number of unique words: {len(self.vocabulary)}\")\n",
    "\n",
    "    def getSentiment(self, tweet) -> list:\n",
    "        \"\"\"\n",
    "        Get the sentiment of the tweet.\n",
    "        \"\"\"\n",
    "        analysis = TextBlob(tweet)\n",
    "        # analyser = SentimentIntensityAnalyzer()\n",
    "        # sent = analyser.polarity_scores(analysis)\n",
    "\n",
    "        return analysis.sentiment.polarity\n",
    "        # return [sent['neg'], sent['neu'], sent['pos'], sent['compound']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 388596\n",
      "Number of unique words: 306797\n"
     ]
    }
   ],
   "source": [
    "processed = process_tweets(RegexpTokenizer(r'\\w+'), \n",
    "                           stop_words=stopwords.words('english'), \n",
    "                           stemmer=SnowballStemmer(\"english\"), \n",
    "                           lemmatizer=WordNetLemmatizer()\n",
    "                           )\n",
    "# Read data and concatenate to dataframe\n",
    "for curr in currency:\n",
    "    processed.read_tweets(curr)\n",
    "\n",
    "processed.clean_df()\n",
    "processed.process_ccy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed.df[0]['text'][:10]\n",
    "# processed.df[0]['text_clean'][:10]\n",
    "\n",
    "# tweet_id,name,screen_name,retweet_count,text,mined_at,created_at,favourite_count,\n",
    "# hashtags,status_count,followers_count,location,source_device,retweet_text\n",
    "for index in range(len(processed.df)):\n",
    "    # Drop columns that are not needed\n",
    "    processed.df[index].drop(columns=['tweet_id', 'name', 'screen_name', 'mined_at',                                      'retweet_count', 'favourite_count', 'hashtags', \n",
    "                                      'status_count', 'followers_count', 'location', \n",
    "                                      'source_device', 'retweet_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  #NEO #btc #ETH \\nPullback to the broken resist...   \n",
      "1  RT @LuckyCorgis: Hey, it's giveaway time!! We'...   \n",
      "2  RT @ICOAnnouncement: ðŸŽ™ New Project\\n\\nðŸ”· SolarD...   \n",
      "3  Feeling like it could finally be my turn to ha...   \n",
      "4  RT @Blockchainsanta: I mean, it is kinda true ...   \n",
      "\n",
      "                  created_at coin_type  \\\n",
      "0  2021-11-14 17:22:12+00:00       BTC   \n",
      "1  2021-11-14 17:22:11+00:00       BTC   \n",
      "2  2021-11-14 17:22:09+00:00       BTC   \n",
      "3  2021-11-14 17:22:08+00:00       BTC   \n",
      "4  2021-11-14 17:22:08+00:00       BTC   \n",
      "\n",
      "                                          text_clean  sentiment  \n",
      "0  neo btc eth pullback broken resistance http co...  -0.400000  \n",
      "1  rt luckycorgis hey giveaway time we giving awa...   0.333333  \n",
      "2  rt icoannouncement new project solardex solar ...   0.136364  \n",
      "3  feeling like could finally turn gotten early m...   0.050000  \n",
      "4  rt blockchainsanta i mean kinda true btc btc h...   0.018750  \n"
     ]
    }
   ],
   "source": [
    "for index in range(len(processed.df)):\n",
    "    processed.df[index]['sentiment'] = processed.df[index]['text_clean'].apply(processed.getSentiment)\n",
    "\n",
    "# Print head of dataframe\n",
    "print(processed.df[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coin_type                                         text_clean  sentiment  \\\n",
      "0       BTC  neo btc eth pullback broken resistance http co...  -0.400000   \n",
      "1       BTC  rt luckycorgis hey giveaway time we giving awa...   0.333333   \n",
      "2       BTC  rt icoannouncement new project solardex solar ...   0.136364   \n",
      "3       BTC  feeling like could finally turn gotten early m...   0.050000   \n",
      "4       BTC  rt blockchainsanta i mean kinda true btc btc h...   0.018750   \n",
      "\n",
      "         date      time  \n",
      "0  2021-11-14  17:22:12  \n",
      "1  2021-11-14  17:22:11  \n",
      "2  2021-11-14  17:22:09  \n",
      "3  2021-11-14  17:22:08  \n",
      "4  2021-11-14  17:22:08  \n"
     ]
    }
   ],
   "source": [
    "# update time format to only take the date\n",
    "for index in range(len(processed.df)):\n",
    "    processed.df[index]['date'] = processed.df[index]['created_at'].apply(lambda x: x[:10])\n",
    "    processed.df[index]['time'] = processed.df[index]['created_at'].apply(lambda x: x[11:19])\n",
    "    processed.df[index].drop(columns=['created_at'], inplace=True)\n",
    "    processed.df[index].drop(columns=['text'], inplace=True)\n",
    "    \n",
    "print(processed.df[0].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start working with Bitcoin Market price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date     close coin_type   time\n",
      "0  2022-01-04  46296.06       BTC  00:00\n",
      "1  2022-01-03  46446.10       BTC  23:00\n",
      "2  2022-01-03  46217.90       BTC  22:00\n",
      "3  2022-01-03  45979.01       BTC  21:00\n",
      "4  2022-01-03  45922.00       BTC  20:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_date(x):\n",
    "    try:\n",
    "        return datetime.strptime(x, '%Y-%m-%d').date()\n",
    "    except:\n",
    "        return None\n",
    "    # return datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "ret = []\n",
    "for curr in currency:\n",
    "    for file in glob.glob(f\"{processed.path}/2022_dataset/{curr}*-Market_Price-*.csv\"):\n",
    "        ret = pd.concat([pd.read_csv(file)], ignore_index=True)\n",
    "        ret['coin_type'] = curr\n",
    "\n",
    "# Process data to only include the date and time\n",
    "ret['time'] = ret['date'].apply(lambda x: x[11:19])\n",
    "ret['date'] = ret['date'].apply(lambda x: x[:10])\n",
    "\n",
    "# Change the format of date in ret from dd/mm/yyyy to yyyy-mm-dd\n",
    "ret['date'] = ret['date'].apply(lambda x: datetime.strptime(x, \"%d/%m/%Y\").strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(ret.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the order of columns \n",
    "market_coin = ret[['coin_type', 'date', 'time', 'close']]\n",
    "# Change name of column 'close' to 'price'\n",
    "market_coin.rename(columns={'close': 'price'}, inplace=True)\n",
    "\n",
    "for index in range(len(processed.df)):\n",
    "    processed.df[index] = processed.df[index][['coin_type', 'text_clean', 'date', 'time', 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating dataframe by hour and then grouping them by hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93263193325cd6f76a9e21d176003adfa7a63d8f308e38218ffc45e6019c4146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
