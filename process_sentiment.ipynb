{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import datetime as dt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pyspark.sql.functions import col, udf, unix_timestamp\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency = [\n",
    "    \"DOGE\"\n",
    "    # \"BTC\",\n",
    "    #\"ETH\",\n",
    "    #\"USDT\",\n",
    "    # \"XRP\",\n",
    "    #\"BCH\",\n",
    "    #\"ADA\",\n",
    "    #\"BSV\",\n",
    "    #\"LTC\",\n",
    "    #\"LINK\",\n",
    "    #\"BNB\",\n",
    "    #\"EOS\",\n",
    "    #\"TRON\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_tweets():\n",
    "\n",
    "    def __init__(self, tokenizer=None, stop_words=None, stemmer=None, lemmatizer=None):\n",
    "        \"\"\" \n",
    "        Initialize the class.\n",
    "        \"\"\"\n",
    "        self.path = Path(f'{os.getcwd()}')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.stemmer = stemmer\n",
    "        self.lemmatizer = lemmatizer\n",
    "\n",
    "        self.df = []\n",
    "        self.vocabulary = []\n",
    "        self.final = []\n",
    "\n",
    "    # Read tweets from CSV for every currency\n",
    "    def read_tweets(self, curr):\n",
    "        \"\"\"\n",
    "        Read the tweets from the CSV file.\n",
    "        \"\"\"\n",
    "        #initialize the dataframe\n",
    "        aux = []\n",
    "        ret = pd.DataFrame()\n",
    "        for file in glob.glob(f\"{self.path}/twitter_data/*/*-{curr}*.csv\"):\n",
    "            # print(f\"Reading {file}\")\n",
    "            ret = pd.concat([pd.read_csv(file)], ignore_index=True)\n",
    "            ret['coin_type'] = curr\n",
    "            aux.append(ret)\n",
    "\n",
    "        self.df = pd.concat(aux, ignore_index=True)\n",
    "        # Print the number of rows\n",
    "        print(f\"Number of rows: {len(self.df)}\")\n",
    "\n",
    "\n",
    "    def clean_df(self):\n",
    "        \"\"\"\n",
    "        Since I repeted the data mining multiple times, we expect duplicate of tweets.\n",
    "        Keep the latess mined as the number of followers and retweets can chage.\n",
    "        \"\"\"\n",
    "        self.df.sort_values(by=['created_at'], ascending=True)\n",
    "        self.df.drop_duplicates(subset=['tweet_id'], keep='last', ignore_index=True)\n",
    "\n",
    "        self.df.drop(columns=['tweet_id', 'name', 'screen_name', 'mined_at', \n",
    "                              'retweet_count', 'favourite_count', 'hashtags', \n",
    "                              'status_count', 'followers_count', 'location', \n",
    "                              'source_device', 'retweet_text'], inplace=True)\n",
    "\n",
    "\n",
    "        self.df['created_at'] = pd.to_datetime(self.df['created_at'], format='%Y-%m-%d %H:%M')\n",
    "        self.df['round_time'] = self.df['created_at'].dt.round('30min')\n",
    "        self.df['round_time'] = self.df['round_time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean the text.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        # Tokenize the text\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        count += len(tokens)\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        # Case Folding\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "        # Lemmatize the text\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # Add word to vocabulary\n",
    "        self.vocabulary.extend(tokens)\n",
    "\n",
    "        # Return the lemma text\n",
    "        return \" \".join(tokens), count\n",
    "\n",
    "    def process_ccy(self):\n",
    "        \"\"\"\n",
    "        Processes the currency data.\n",
    "        \"\"\"\n",
    "        # Create a new column for text_clean and assign \"\"\n",
    "        self.df[\"text_clean\"] = \"\"\n",
    "\n",
    "        count = 0\n",
    "        # Create column for text_clean and process the text\n",
    "        for index, row in self.df.iterrows():\n",
    "\n",
    "            # Clean the text\n",
    "            text_clean, words = self.clean_text(row['text'])\n",
    "            count += words\n",
    "            # Add the cleaned text to the dataframe\n",
    "            self.df['text_clean'][index] = text_clean\n",
    "        \n",
    "        print(f\"There are {count} words in the text\")\n",
    "        print(f\"Number of unique words: {len(self.vocabulary)}\")\n",
    "    \n",
    "    def drop_last(self):\n",
    "        \"\"\"\n",
    "        Drop text and created_at columns.\n",
    "        \"\"\"\n",
    "        self.df.drop(columns=['text', 'created_at'], inplace=True)\n",
    "\n",
    "\n",
    "    def getSentiment(self, tweet) -> list:\n",
    "        \"\"\"\n",
    "        Get the sentiment of the tweet.\n",
    "        \"\"\"\n",
    "        analysis = TextBlob(tweet)\n",
    "\n",
    "        return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 66251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulc\\AppData\\Local\\Temp/ipykernel_348/369241009.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['text_clean'][index] = text_clean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1398224 words in the text\n",
      "Number of unique words: 1134537\n"
     ]
    }
   ],
   "source": [
    "processed = process_tweets(RegexpTokenizer(r'\\w+'), \n",
    "                           stop_words=stopwords.words('english'), \n",
    "                           stemmer=SnowballStemmer(\"english\"), \n",
    "                           lemmatizer=WordNetLemmatizer()\n",
    "                           )\n",
    "# Read data and concatenate to dataframe\n",
    "for curr in currency:\n",
    "    processed.read_tweets(curr)\n",
    "\n",
    "processed.clean_df()\n",
    "processed.process_ccy()\n",
    "processed.drop_last()\n",
    "\n",
    "# Create a dataframe with the sentiment\n",
    "for curr in currency:\n",
    "    processed.df['sentiment'] = processed.df['text_clean'].apply(processed.getSentiment)\n",
    "\n",
    "# Rearange the columns 'coin_type', 'round_time', 'text_clean', 'sentiment'\n",
    "processed.df = processed.df[['coin_type', 'round_time', 'text_clean', 'sentiment']]\n",
    "\n",
    "\n",
    "# Print head of the dataframe\n",
    "# print(f\"Head of the dataframe: {processed.df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating dataframe by hour and then grouping them by hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of the hourly sentiment dataframe:                hour  sentiment\n",
      "0  2022-01-14 08:00   0.171108\n",
      "1  2022-01-14 07:30   0.165510\n",
      "2  2022-01-14 07:00   0.124129\n",
      "3  2022-01-14 06:30   0.109723\n",
      "4  2022-01-14 06:00   0.135061\n",
      "Number of rows: 59\n"
     ]
    }
   ],
   "source": [
    "hourly_sentiment_df = pd.DataFrame()\n",
    "\n",
    "# for loop to get the mean of sentiment for each hour\n",
    "for hour in processed.df['round_time'].unique():\n",
    "    # Get the sentiment for each hour\n",
    "    sentiment = processed.df[processed.df['round_time'] == hour]['sentiment']\n",
    "    mean_sentiment = sentiment.mean()\n",
    "\n",
    "    # Create a dataframe with the mean sentiment for each hour\n",
    "    hourly_sentiment_df = hourly_sentiment_df.append({'hour': hour, 'sentiment': mean_sentiment}, ignore_index=True)\n",
    "\n",
    "print(f\"Head of the hourly sentiment dataframe: {hourly_sentiment_df.head()}\")\n",
    "print(f\"Number of rows: {len(hourly_sentiment_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Market Price for #COINS to the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_date(x):\n",
    "    try:\n",
    "        return datetime.strptime(x, '%Y-%m-%d').date()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "ret = []\n",
    "for curr in currency:\n",
    "    for file in glob.glob(f\"{processed.path}/market-price_data/{curr}*.csv\"):\n",
    "        ret = pd.concat([pd.read_csv(file)], ignore_index=True)\n",
    "        ret['coin_type'] = curr\n",
    "\n",
    "# Process TIME column to create 2 columns in dataframe with date and time\n",
    "ret['date'] = ret['TIME'].apply(lambda x: x[:8])\n",
    "# Change the format of date in ret from dd-mm-yy to yyyy-mm-dd\n",
    "ret['date'] = ret['date'].apply(lambda x: datetime.strptime(x, '%d-%m-%y').date())\n",
    "\n",
    "ret['time'] = ret['TIME'].apply(lambda x: x[9:14])\n",
    "ret['time'] = ret['time'].apply(lambda x: datetime.strptime(x, '%H:%M').time())\n",
    "\n",
    "# Drop columns that are not needed PAIR, TIME\n",
    "ret.drop(columns=['PAIR', 'TIME'], inplace=True)\n",
    "# Rearange columns to match the dataframe\n",
    "ret = ret[['coin_type', 'date', 'time', 'BID', 'ASK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coin_type        date      time      BID      ASK\n",
      "0      DOGE  2022-01-15  23:30:00  0.18476  0.18488\n",
      "1      DOGE  2022-01-15  23:00:00  0.18473  0.18483\n",
      "2      DOGE  2022-01-15  22:30:00  0.18569  0.18570\n",
      "3      DOGE  2022-01-15  22:00:00  0.18635  0.18641\n",
      "4      DOGE  2022-01-15  21:30:00  0.18673  0.18685\n"
     ]
    }
   ],
   "source": [
    "print(ret.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the order of columns \n",
    "# market_coin = ret[['coin_type', 'date', 'time', 'close']]\n",
    "# Change name of column 'close' to 'price'\n",
    "# market_coin.rename(columns={'close': 'price'}, inplace=True)\n",
    "\n",
    "for index in range(len(processed.df)):\n",
    "    processed.df[index] = processed.df[index][['coin_type', 'date', 'time', 'text_clean', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  coin_type        date      time  \\\n",
      "0      DOGE  2022-01-14  15:01:50   \n",
      "1      DOGE  2022-01-14  15:01:50   \n",
      "2      DOGE  2022-01-14  15:01:47   \n",
      "3      DOGE  2022-01-14  15:01:45   \n",
      "4      DOGE  2022-01-14  15:01:43   \n",
      "\n",
      "                                          text_clean  sentiment  \n",
      "0  rt bleufiofficial just 2 day launching happy a...   0.350000  \n",
      "1  excellent trx mining signup bonus 3000 tron bt...   1.000000  \n",
      "2  rt dogesecurity1 this real magic doge rt u dog...   0.750000  \n",
      "3  rt kikuinubsc kikuarmy hitting that 1 98m mark...   0.068182  \n",
      "4  rt taylormusk_ the next gem x100 bnb usdt doge...   0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(processed.df[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one big dataframe with all the data concatenated together\n",
    "# df_final = []\n",
    "# for index in range(len(processed.df)):\n",
    "#     df_final = pd.concat([processed.df[index]])\n",
    "\n",
    "# # print(df_final.head())\n",
    "\n",
    "# # Group the data by date_time\n",
    "# df_final = df_final.groupby(['date_time', 'sentiment', 'coin_type']).mean()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93263193325cd6f76a9e21d176003adfa7a63d8f308e38218ffc45e6019c4146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
